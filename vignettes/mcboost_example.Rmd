---
title: "MCBoost - Health Survey Example"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{MCBoost - Example}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(tidyverse)
library(purrr)
library(PracTools)
library(randomForest)
library(mlr3)
library(mlr3learners)
library(classifierplots)
library(mcboost)
```

## Data

```{r}
data(nhis.large)
```

```{r}
predictors <- c("sex", "age.grp", "hisp", "parents", "educ", "race", "inc.grp", "doing.lw", "limited")

nhis <- nhis.large %>%
  mutate_at(predictors, as_factor) %>%
  mutate_at(predictors, fct_explicit_na) %>%
  drop_na(notcov) %>%
  select(all_of(predictors), notcov, stratum, svywt)

nhis$notcov <- factor(ifelse(nhis$notcov == 1, "notcov", "cov"))

nhis_enc <- data.frame(model.matrix(notcov ~ . -sex -hisp -race -stratum, data = nhis)[,-1])
nhis_enc$notcov <- nhis$notcov
nhis_enc$sex <- nhis$sex
nhis_enc$hisp <- nhis$hisp
nhis_enc$race <- nhis$race
nhis_enc$stratum <- nhis$stratum
nhis_enc$svywt <- nhis$svywt
```

```{r}
nhis_enc_g <- nhis_enc %>% 
  group_by(stratum) %>%
  nest() %>%
  mutate(mean_wgt = map(data, ~mean(.x$svywt))) %>%
  ungroup()
```

```{r}
test_g <- nhis_enc_g %>% slice_sample(prop = 0.25, weight_by = mean_wgt)

nontest_g <- nhis_enc_g %>% anti_join(test_g, by = "stratum")

train_g <- nontest_g %>% slice_sample(prop = 0.7)

post <- nontest_g %>% 
  anti_join(train_g, by = "stratum") %>% 
  unnest(data) %>% 
  select(-mean_wgt, -svywt, -stratum, -sex, -race, -hisp)

train <- train_g %>% unnest(data) %>% select (-mean_wgt, -svywt, -stratum, -sex, -race, -hisp)
test <- test_g %>% unnest(data) %>% select (-mean_wgt)
```

## Initial model

```{r}
set.seed(23)
rf <- randomForest(notcov ~ ., 
                   data = train)
```

## MCBoost

```{r}
init_predictor = function(data) {
  predict(rf, data, type = "prob")[, 2]
}
```

```{r}
d <- select(post, -notcov)
l <- 1 - one_hot(post$notcov)
```

```{r}
ridge = LearnerResidualFitter$new(lrn("regr.glmnet", alpha = 0, lambda = 2 / nrow(post)))
```

```{r}
mc = MCBoost$new(init_predictor = init_predictor, 
                 subpop_fitter = ridge,
                 multiplicative = TRUE,
                 partition = TRUE,
                 max_iter = 15)
mc$multicalibrate(d, l)
```

## Evaluate

```{r}
test$naive <- predict(rf, newdata = test, type = "prob")[, 2]
test$mcb <- mc$predict_probs(test)

test$c_naive <- round(test$naive)
test$c_mcb <- round(test$mcb)
test$label <- 1 - one_hot(test$notcov)
```

```{r}
mean(test$c_naive == test$label)
mean(test$c_mcb == test$label)

# classifierplots(test$label, test$naive)
# classifierplots(test$label, test$mcb)
```

```{r}
test <- test %>% 
  mutate(sex_hisp = group_indices(., sex, hisp),
         sex_race = group_indices(., sex, race),
         hisp_race = group_indices(., hisp, race))

grouping_vars <- c("sex", "hisp", "race", "sex_hisp", "sex_race", "hisp_race", "stratum")

eval <- map(grouping_vars, group_by_at, .tbl = test) %>%
  map(summarise,
      'accuracy_rf' = mean(c_naive == label),
      'accuracy_mcb' = mean(c_mcb == label),
      'bias_rf' = (mean(naive) - mean(label))*100,
      'bias_mcb' = (mean(mcb) - mean(label))*100,
      'size' = n()) %>%
  bind_rows()
```

```{r}
eval_long <- eval %>%
 pivot_longer(
   cols = accuracy_rf:bias_mcb,
   names_to = c("type", "model"),
   names_sep = "_") %>%
  arrange(desc(size))

eval_long$id <- rep(1:(nrow(eval_long)/4), each = 4)
```

```{r}
eval_long %>%
  filter(type == "bias" & size >= 50) %>%
  ggplot() +
  geom_line(aes(id, value, color = model)) + 
  geom_hline(yintercept = 0, linetype = "dashed")
```

