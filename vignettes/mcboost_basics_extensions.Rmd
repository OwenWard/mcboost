---
title: "MCBoost - Basics and Extensions"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{MCBoost - Basics and Extensions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(mcboost)
library(mlr3)
set.seed(83007)
```


## Example 0: Multi-Calibration in 6 Lines of code

As a brief introduction we show how to use **mcboost** in only 6 lines of code.
For our example, we use the data from the *sonar* binary classification task.
We instantiate a `MCBoost` instance by specifying a `subpop_fitter`.
This `subpop_fitter` defines the splits into groups in each boosting iteration
based on the obtained residuals. 
In this example, we choose a `Tree` based model. 
Afterwards, we run the `$multicalibrate()` method on our data to start multi-calibration.
We only use the first 200 samples of the *sonar* data set to train our multi-calibrated model.

```{r}
tsk = tsk("sonar")
d = tsk$data(cols = tsk$feature_names)
l = tsk$data(cols = tsk$target_names)[[1]]
mc = MCBoost$new(subpop_fitter = "TreeResidualFitter")
mc$multicalibrate(d[1:200,], l[1:200])
```

After the calibration, we use the model to predict on the left-out data (8 observations).

```{r}
mc$predict_probs(d[201:208,])
```

## Example 1: Multi-Calibration Boosting on the Adult Dataset

First we download the data and create an `mlr3` classification task:

```{r}
library(data.table)
adult_train = fread(
  "https://raw.githubusercontent.com/Yorko/mlcourse.ai/master/data/adult_train.csv",
  stringsAsFactors = TRUE
)
# adult_train = adult_train[, c("Age", "Workclass", "Education", "Martial_Status", "Occupation", "Relationship", "Race", "Capital_Gain", 
#   "Hours_per_week", "Country", "Target")]
train_tsk = TaskClassif$new("adult_train", adult_train, target = "Target")
```

### Preprocessing

Then we do basic preprocessing: 

  * Collapse rarest factors according to their prevalence 
  * Drop missing factor levels
  * One-hot encode categorical variables
  * Impute NA's using a histogram approach

```{r}
library(mlr3pipelines)
pipe = po("collapsefactors", no_collapse_above_prevalence = 0.0006) %>>%
  po("fixfactors") %>>%
  po("encode") %>>%
  po("imputehist")
prep_task = pipe$train(train_tsk)[[1]]
```

Now we fit our first `Learner`: A `random forest`.

```{r}
library(mlr3learners)
l = lrn("classif.ranger", num.trees = 10L, predict_type = "prob")
l$train(prep_task)
```

### MCBoost

A simple way to use the predictions from any `Model` in **mcboost** is to wrap the predict
function and provide it as an initial predictor. This can be done from any model / any library.
Note, that we have to make sure, that our `init_predictor` returns a numeric vector of predictions.

```{r}
init_predictor = function(data) {
  l$predict_newdata(data)$prob[, 1]
}
```

As **mcboost** requires the data to be provided in `X, y` format (a `data.table` or `data.frame` of features and a
vector of labels), we create those two objects.

```{r}
data = prep_task$data(cols = prep_task$feature_names)
labels = prep_task$data(cols = prep_task$target_names)[[1]]
```

```{r}
mc = MCBoost$new(subpop_fitter = "TreeResidualFitter", init_predictor = init_predictor)
mc$multicalibrate(data, labels)
```

### Evaluation on Test Data

```{r}
adult_test = fread(
  "https://raw.githubusercontent.com/Yorko/mlcourse.ai/master/data/adult_test.csv",
  stringsAsFactors = TRUE
)
# The first row seems to have an error
adult_test = adult_test[Target != "",]

# Note, that we have to convert columns from numeric to integer here:
sdc = train_tsk$feature_types[type == "integer", id]
adult_test[, (sdc) := lapply(.SD, as.integer), .SDcols = sdc]
# adult_test = adult_test[, c("Age", "Workclass", "Education", "Martial_Status", "Occupation", "Relationship", "Race", "Capital_Gain", 
#   "Hours_per_week", "Country", "Target")]

test_tsk = TaskClassif$new("adult_test", adult_test, target = "Target")
prep_test = pipe$predict(test_tsk)[[1]]
```

Now, we can again extract `X,y`.

```{r}
test_data = prep_test$data(cols = prep_test$feature_names)
test_labels = prep_test$data(cols = prep_test$target_names)[[1]]
```

and **predict**.

```{r}
prs = mc$predict_probs(test_data)
```

Now we can compute the accuracy of the multi-calibrated model

```{r}
mean(round(prs) == one_hot(test_labels)[, 2])
```

and compare to the non-calibrated version:

```{r}
mean(round(init_predictor(test_data)) == one_hot(test_labels)[, 2])
```

but looking at sub-populations we can see that the predictions got 
more calibrated. Since we cannot show all subpopulations we only show the accuracy 
for to the feature  `Education`. 
We can see that 

```{r}
# Get accuracy per subgroup for multi-calibrated predictor
adult_test$accmc = (prs - one_hot(test_labels)[, 2])
adult_test[, .(abs(mean(accmc)), .N), by = .(Race)]
# Get accuracy per subgroup for initial predictor
adult_test$accinit = (init_predictor(test_data) - one_hot(test_labels)[, 2])
adult_test[, .(abs(mean(accinit)), .N), by = .(Race)]

# mask = as.logical(test_data$Race.Black)
# mean(prs[mask] == one_hot(test_labels)[mask])
# mean(init_predictor(test_data)[mask] == one_hot(test_labels)[mask])
```


### The Auditor effect

We can also obtain the auditor effect after multicalibration.
This indicates "how much" each observation has been affected by multi-calibration (on average across iterations).

```{r}
ae = mc$auditor_effect(test_data)
hist(ae)
```

We can see that there is a pronounced effect for a few instances, while for most, there is no actual effect.
Let's investigate those instances!


In order to get some insights, we compute quantiles of the
effected and un-effected population and analyse differences.

```{r}
effect = apply(test_data[ae >= median(ae[ae>0]),], 2, quantile)
no_effect  = apply(test_data[ae <= 0.01,], 2, quantile)
difference = apply((effect-no_effect), 2, mean)
difference[difference > 0]
```

There seems to be a difference in some variables like `Age` and `Sex.Male`. We can thus assume, that our auditing affected mostly males, that are on average older, married and have a higher education.

We can further analyze the individuals:

```{r}
test_data[ae >= median(ae[ae>0]), names(which(difference > 0)), with = FALSE]
```

### Predicting using only the first 'n' iterations

The `t` parameter can be use to predict using only the first `t` iterations.

```{r}
prs = mc$predict_probs(test_data, t = 3L)
```


## Example 2: MCBoost with non-mlr3 models: GLM

`mcboost` does not require your model to be a `mlr3` model.
As an input, `mcboost` expects a function `init_predictor` that takes as input `data` and returns a prediction.


```{r}
tsk = tsk("sonar")
data = tsk$data()[, Class := as.integer(Class) - 1L]
mod = glm(data = data, formula = Class ~ .)
```

The `init_predictor` could then use the `glm` model:

```{r}
init_predictor = function(data) {
  predict(mod, data)
}
```

... and we can calibrate this predictor.

```{r}
d = data[, -1]
l = data$Class
mc = MCBoost$new(init_predictor = init_predictor)
mc$multicalibrate(d[1:200,], l[1:200])
mc$predict_probs(d[201:208,])
```


### Example 3: Avoiding Overfitting in MCBoost

Very often `MCBoost`'s calibration is very aggressive and tends to overfit.
This section tries to introduce a method to regularize against this overfitting.

### 3.1 CVLearner

In this section we use a
`Cross-Validated` learner that predicts on held-out data during the training phase. This idea is based on Wopert (1992)'s Stacked Generalization.
Other, simpler methods include choosing a smaller step size `eta` or reducing the number of `iters`.

```{r}
tsk = tsk("sonar")
```

As an `init_predictor` we again use a `ranger` model from mlr3 and
construct an init predictor using the convenience function provided by `mcboost`.

```{r}
learner = lrn("classif.ranger", predict_type = "prob")
learner$train(tsk)
init_predictor = mlr3_init_predictor(learner)
```

... and we can calibrate this predictor.
This time, we use a `CVTreeResidualFitter` instead of a `TreeResidualFitter`. This allows us to avoid
overfitting similar to a technique coined `stacked generalization` first described by Wolpert in 1992.
Note, that this can sometimes take a little longer since each learner is cross-validated using `3` folds (default).

```{r}
d = data[, -1]
l = data$Class
mc = MCBoost$new(init_predictor = init_predictor, subpop_fitter=CVTreeResidualFitter$new(), max_iter = 2L)
mc$multicalibrate(d[1:200,], l[1:200])
mc$predict_probs(d[201:208,])
```


### 3.2 Data Splitting

We can also use a fresh chunk of the validation data in each iteration. `mcboost` implements two strategies, `"bootstrap"` and `"split"`. While `"split"` simply splits up the data,  `"bootstrap"` draws a new bootstrap sample of the data in each iteration.

```{r}
tsk = tsk("sonar")
```

Again, we use a `ranger` mlr3 model as our initial predictor:

```{r}
learner = lrn("classif.ranger", predict_type = "prob")
learner$train(tsk)
init_predictor = mlr3_init_predictor(learner)
```

and we can now calibrate: 

```{r}
d = data[, -1]
l = data$Class
mc = MCBoost$new(
  init_predictor = init_predictor,
  subpop_fitter= TreeResidualFitter$new(),
  iter_sampling = "bootstrap"
)
mc$multicalibrate(d[1:200,], l[1:200])
mc$predict_probs(d[201:208,])
```


## Example 4: Adjusting the SubPop Fitter

For this example, we use the *sonar* dataset once again:

```{r}
tsk = tsk("sonar")
data = tsk$data(cols = tsk$feature_names)
labels = tsk$data(cols = tsk$target_names)[[1]]
```

### 2.1 LearnerResidualFitter

The Subpop-fitter can be easily adjusted by constructing it from a `LearnerResidualFitter`.
This allows for using any **mlr3** learner.
See [here](https://mlr3extralearners.mlr-org.com/articles/learners/list_learners.html) for a list of available learners.


```{r}
rf = LearnerResidualFitter$new(lrn("regr.rpart", minsplit = 10L))
mc = MCBoost$new(subpop_fitter = rf)
mc$multicalibrate(data, labels)
```

The `TreeResidualFitter` and `RidgeResidualFitter` are two instantiations of this Fitter with pre-defined learners. By providing their character strings the fitter could be automatically constructed.


### 2.2 SubpopResidualFitter & SubgroupResidualFitter

In some occasions, instead of using a `Learner`, we might want to use a fixed set of subgroups.
Those can either be defined from the data itself or provided from the outside.

**Splitting via the dataset**

In order to split the data into groups according to a set of columns, we use a `SubpopFitter`
together with a list of `subpops`. Those define the group splits to multi-calibrate on.
These splits can be either a `character` string, referencing a binary variable in the data
or a `function` that, when evaluated on the data, returns a binary vector.

In order to showcase both options, we add a binary variable to our `data`:

```{r}
data[, Bin := sample(c(1, 0), nrow(data), replace = TRUE)]
```

```{r}
rf = SubpopFitter$new(list(
  "Bin",
  function(data) {data[["V1"]] > 0.2},
  function(data) {data[["V1"]] > 0.2 | data[["V3"]] < 0.29}
))
```

```{r}
mc = MCBoost$new(subpop_fitter = rf)
mc$multicalibrate(data, labels)
```

And we can again apply it to predict on new data:

```{r}
mc$predict_probs(data)
```

**Manually defined masks**

If we want to add the splitting from the outside, by supplying binary masks for the
rows of the data, we can provide manually defined masks. 
Note, that the masks have to correspond with the number of rows in the dataset.

```{r}
rf = SubgroupFitter$new(list(
  rep(c(0, 1), 104),
  rep(c(1, 1, 1, 0), 52)
))
```

```{r}
mc = MCBoost$new(subpop_fitter = rf)
mc$multicalibrate(data, labels)
```

During prediction, we now have to supply a set of masks for the prediction data.

```{r}
predict_masks = list(
  rep(c(0, 1), 52),
  rep(c(1, 1, 1, 0), 26)
)
```

```{r}
mc$predict_probs(data[1:104,], subgroup_masks = predict_masks)
```
