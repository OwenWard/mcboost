---
title: "MCBoost - Basics and Extensions"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{MCBoost - Basics and Extensions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(mcboost)
library(mlr3)
```


## Example 0: Multi-Callibration in 6 Lines of code

As a brief introduction we show how to use **mcboost** in only 6 lines of code.
For our example, we use the data from the *sonar* binary classification task.
We instantiate a `MCBoost` instance, specifying a `subpop_fitter`.
This `subpop_fitter` defines the splits into groups in each boosting iteration.
In this example, we choose a `Tree` based model.
Afterwards, we run the `$multicalibrate()` method on our  data to start multi-calibration.

```{r}
tsk = tsk("sonar")
d = tsk$data(cols = tsk$feature_names)
l = tsk$data(cols = tsk$target_names)[[1]]
mc = MCBoost$new(subpop_fitter = "TreeResidualFitter")
mc$multicalibrate(d[1:200,], l[1:200])
```

After the calibration, the model can now be used for prediction with new data.

```{r}
  mc$predict_probs(d[201:208,])
```

## Example 1: Multi-Callibration Boosting on the Adult Dataset

First we download the data:

```{r}
library(data.table)
adult_train = fread(
  "https://raw.githubusercontent.com/Yorko/mlcourse.ai/master/data/adult_train.csv",
  stringsAsFactors = TRUE
)
train_tsk = TaskClassif$new("adult_train", adult_train, target = "Target")
```

### Preprocessing

Then we do basic preprocessing:
- Drop missing factor levels
- Impute NA's using a histogram approach
- One-hot encode categorical variables

```{r}
library(mlr3pipelines)
pipe = po("collapsefactors", no_collapse_above_prevalence = 0.0006) %>>%
  po("fixfactors") %>>%
  po("encode") %>>% po("imputehist")
prep_task = pipe$train(train_tsk)[[1]]
```

Now we fit our first `Learner`: A `random forest`.

```{r}
library(mlr3learners)
l = lrn("classif.ranger", num.trees = 50L)
l$train(prep_task)
```

### MCBoost
A simple way to use the predictions from any `Model` in **mcboost** is to wrap the predict
function and provide it as an initial predictor. This can be done from any model / any library.
Note, that we have to make sure, that our `init_predictor` returns a numeric vector of predictions.

```{r}
init_predictor = function(data) {
  one_hot(l$predict_newdata(data)$response)
}
```

As **mcboost** requires the data to be provided in `X, y` format (a data.table of features and a
vector of labels), we create those two objects.

```{r}
data = prep_task$data(cols = prep_task$feature_names)
labels = prep_task$data(cols = prep_task$target_names)[[1]]
```

```{r}
mc = MCBoost$new(subpop_fitter = "TreeResidualFitter", init_predictor = init_predictor)
mc$multicalibrate(data, labels)
```

### Evaluation on Test Data

```{r}
adult_test = fread(
  "https://raw.githubusercontent.com/Yorko/mlcourse.ai/master/data/adult_test.csv",
  stringsAsFactors = TRUE
)
# The first row seems to have an error
adult_test = adult_test[Target != "",]

# Note, that we have to convert columns from numeric to integer here:
sdc = train_tsk$feature_types[type == "integer", id]
adult_test[, (sdc) := lapply(.SD, as.integer), .SDcols = sdc]

test_tsk = TaskClassif$new("adult_test", adult_test, target = "Target")
prep_test = pipe$predict(test_tsk)[[1]]
```

Now, we can again extract `X,y`.

```{r}
test_data = prep_test$data(cols = prep_test$feature_names)
test_labels = prep_test$data(cols = prep_test$target_names)[[1]]
```
and **predict**.

```{r}
prs = mc$predict_probs(test_data)
prs = round(prs)
```

Now we can compute the accuracy of the multi-calibrated model

```{r}
mean(prs == one_hot(test_labels))
```

and compare to the non-calibrated version:

```{r}
mean(init_predictor(test_data) == one_hot(test_labels))
```

but looking at sub-populations:

```{r}
mask = test_data$Race.Black
mean(prs[mask] == one_hot(test_labels)[mask])
mean(init_predictor(test_data)[mask] == one_hot(test_labels)[mask])
```

## Example 2: Adjusting the SubPop Fitter

For this example, we will again use the *sonar* dataset:

```{r}
  tsk = tsk("sonar")
  d = tsk$data(cols = tsk$feature_names)
  l = tsk$data(cols = tsk$target_names)[[1]]
```

### 2.1 LearnerResidualFitter

The Subpop-fitter can be easily adjusted by constructing it from a `LearnerResidualFitter`.
This allows for using any **mlr3** learner.
See [here](https://mlr3extralearners.mlr-org.com/articles/learners/list_learners.html) for a list of available learners.


```{r}
rf = LearnerResidualFitter$new(lrn("regr.rpart", minsplit = 10L))
mc = MCBoost$new(subpop_fitter = rf, init_predictor = init_predictor)
mc$multicalibrate(data, labels)
```

The `TreeResidualFitter` and `RidgeResidualFitter` are two instantiations of this Fitter with pre-defined learners. By providing either as a character string, the fitter is automatically constructed.


### 2.2 SubpopResidualFitter & SubgroupResidualFitter

In some occasions, instead of using a `Learner`, we might want to use a fixed set of subgroups.
Those can either be defined from the data itself or provided from the outside.

**Splitting via the dataset**

In order to split the data into groups according to a set of columns, we use a `SubpopFitter`
together with a list of `subpops`; Those define the group splits to multi-calibrate on.
These splits can be either a `character` string, referencing a binary variable in the data
or a `function` that, when evaluated on the data, returns a binary vector.


In order to showcase both options, we add a binary variable to our `data`:

```{r}
data[, Bin := sample(c(1,0), nrow(data), replace = TRUE)]
```

```{r}
rf = SubpopFitter$new(list(
  "Bin",
  function(data) {data[["V1"]] > 0.2},
  function(data) {data[["V1"]] > 0.2 | data[["V3"]] < 0.29}
))
```

```{r}
mc = MCBoost$new(subpop_fitter = rf)
mc$multicalibrate(data, labels)
```

And we can now, again use it to predict on new data:

```{r}
mc$predict_probs(data)
```

**Manually defined masks**

If we do not want to add the splitting from the outside, by supplying binary masks for the
rows of the data.
Note, that the masks have to correspond with the number of rows in the dataset.

```{r}
rf = SubgroupFitter$new(list(
  rep(c(0,1), 104),
  rep(c(1,1,1,0), 52)
))
```

```{r}
mc = MCBoost$new(subpop_fitter = rf)
mc$multicalibrate(data, labels)
```

During prediction, we now have to supply a set of masks for the prediction data.

```{r}
predict_masks = list(
  rep(c(0,1), 52),
  rep(c(1,1,1,0), 26)
)
```

```{r}
mc$predict_probs(data[1:104,], subgroup_masks = predict_masks)
```