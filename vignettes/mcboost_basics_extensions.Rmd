---
title: "MCBoost - Basics and Extensions"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{MCBoost - Basics and Extensions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(mcboost)
```


## Example 1: Multi-Callibration Boosting on the Adult Dataset

First we download the data:

```{r}
library(mlr3)
library(data.table)
adult_train = fread(
  "https://raw.githubusercontent.com/Yorko/mlcourse.ai/master/data/adult_train.csv",
  stringsAsFactors = TRUE
)
train_tsk = TaskClassif$new("adult_train", adult_train, target = "Target")
```

### Preprocessing

Then we do basic preprocessing:
- Drop missing factor levels
- Impute NA's using a histogram approach
- One-hot encode categorical variables

```{r}
library(mlr3pipelines)
pipe = po("collapsefactors", no_collapse_above_prevalence = 0.0006) %>>%
  po("fixfactors") %>>%
  po("encode") %>>% po("imputehist")
prep_task = pipe$train(train_tsk)[[1]]
```

Now we fit our first `Learner`: A `random forest`.

```{r}
library(mlr3learners)
l = lrn("classif.ranger", num.trees = 50L)
l$train(prep_task)
```

### MCBoost
A simple way to use the predictions from any `Model` in **mcboost** is to wrap the predict
function and provide it as an initial predictor. This can be done from any model / any library.
Note, that we have to make sure, that our `init_predictor` returns a numeric vector of predictions.

```{r}
init_predictor = function(data) {
  one_hot(l$predict_newdata(data)$response)
}
```

As **mcboost** requires the data to be provided in `X, y` format (a data.table of features and a
vector of labels), we create those two objects.

```{r}
data = prep_task$data(cols = prep_task$feature_names)
labels = prep_task$data(cols = prep_task$target_names)[[1]]
```

```{r}
mc = MCBoost$new(subpop_fitter = "TreeResidualFitter", init_predictor = init_predictor)
mc$multicalibrate(data, labels)
```

### Evaluation on Test Data

```{r}
adult_test = fread(
  "https://raw.githubusercontent.com/Yorko/mlcourse.ai/master/data/adult_test.csv",
  stringsAsFactors = TRUE
)
# The first row seems to have an error
adult_test = adult_test[Target != "",]

# Note, that we have to convert columns from numeric to integer here:
sdc = train_tsk$feature_types[type == "integer", id]
adult_test[, (sdc) := lapply(.SD, as.integer), .SDcols = sdc]

test_tsk = TaskClassif$new("adult_test", adult_test, target = "Target")
prep_test = pipe$predict(test_tsk)[[1]]
```

Now, we can again extract `X,y`.

```{r}
test_data = prep_test$data(cols = prep_test$feature_names)
test_labels = prep_test$data(cols = prep_test$target_names)[[1]]
```
and **predict**.

```{r}
prs = mc$predict_probs(test_data)
prs = round(prs)
```

Now we can compute the accuracy of the multi-calibrated model

```{r}
mean(prs == one_hot(test_labels))
```

and compare to the non-calibrated version:

```{r}
mean(init_predictor(test_data) == one_hot(test_labels))
```

but looking at sub-populations:

```{r}
mask = test_data$Race.Black
mean(prs[mask] == one_hot(test_labels)[mask])
mean(init_predictor(test_data)[mask] == one_hot(test_labels)[mask])
```

## Example 2: Adjusting the SubPop Fitter

The Subpop-fitter can be easily adjusted by constructing it from a `LearnerResidualFitter`.
This allows for using any **mlr3** learner.
See [here](https://mlr3extralearners.mlr-org.com/articles/learners/list_learners.html) for a list of available learners.

```{r}
rf = LearnerResidualFitter$new(lrn("regr.rpart", minsplit = 10L))
mc = MCBoost$new(subpop_fitter = rf, init_predictor = init_predictor)
mc$multicalibrate(data, labels)
```
